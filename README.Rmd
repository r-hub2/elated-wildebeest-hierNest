---
title: "Guidance of hierNest version 1"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```



```{r}
library(hierNest)
library(rTensor)
sp=sample(1:NROW(data$X))
fit1=hierNest::hierNest(data$X[sp,],
                        data$Y[sp],
                        method="overlapping",  ## Overlapping group lasso method
                        hier_info=data$hier_info,  ## Should input the hierarchical information for the groups
                        random_asparse = FALSE,  ## Randomly draw the other two tuning parameter?
                        nlambda=100,
                   #     lambda=4.574160e-08,
                        asparse1 = rep(1000000,100),
                        asparse2 = rep(1000000,100),
                        intercept = FALSE,  ## Set intercept = FALSE can potentially save a huge amount of time
                        family="gaussian")
#fit1$beta
apply(fit1$beta,2,function(x){
  sum(x!=0)
})
fit1$df
fit1$beta[,2]
# 
# fit1$beta[,1]
# 
# fit1$beta[,2]
# 
# fit1$beta[,3]

fit1$lambda




fit3=hierNest::hierNest(data$X[sp,],
                        data$Y[sp],
                        method="overlapping",  ## Overlapping group lasso method
                        hier_info=data$hier_info,  ## Should input the hierarchical information for the groups
                        random_asparse = FALSE,  ## Randomly draw the other two tuning parameter?
                       lambda = fit1$lambda,
                   #     lambda=4.574160e-08,
                        asparse1 = rep(100,100),
                        asparse2 = rep(100,100),
                        intercept = FALSE,  ## Set intercept = FALSE can potentially save a huge amount of time
                        family="binomial")
fit3$df

fit3$lambda






fit2=hierNest::hierNest(data$X[sp,],
                        data$Y[sp],
                        method="overlapping",  ## Overlapping group lasso method
                        hier_info=data$hier_info,  ## Should input the hierarchical information for the groups
                        random_asparse = FALSE,  ## Randomly draw the other two tuning parameter?
                        nlambda=100,
                        asparse1 = rep(1,100),
                        asparse2 = rep(1,100),
                        intercept = FALSE,  ## Set intercept = FALSE can potentially save a huge amount of time
                        family="binomial")

fit2$lambda

apply(fit2$beta,2,function(x){
  sum(x!=0)
})
fit2$df
fit2$beta[64,]
```



```{r}

library(hierNest)
library(rTensor)
## Load the example data with 4 MDC groups with 4 DRGs for each MDC. 
data=readRDS("./example_data.Rdata")


tt1=Sys.time()
fit1=hierNest::hierNest(data$X,
                        data$Y,
                        method="overlapping",  ## Overlapping group lasso method
                        hier_info=data$hier_info,  ## Should input the hierarchical information for the groups
                        random_asparse = TRUE,  ## Randomly draw the other two tuning parameter?
                        nlambda=100,
                        intercept = FALSE,  ## Set intercept = FALSE can potentially save a huge amount of time
                        family="binomial")
tt2=Sys.time()
print(tt2-tt1)


sample_inx=sort(sample(1:NROW(data$X),NROW(data$X),TRUE))

## For now, please use our unique function of predict_hierNest to predict the outcome.
pred1=hierNest::predict_hierNest(fit1,newx = data$X[sample_inx,],hier_info=data$hier_info[sample_inx,],type = "response")

pred1[,80]
```


##  The following show two ways of cross-validation 
Since our model have three tuning parameters $\lambda, \alpha_1, \alpha_2$ that need to be selected through cross-validation, the following implements two ways of cross-validation. 

### Method 1: general cross-validation method 
The first method randomly select $\alpha_1, \alpha_2$ corresponding to each $\lambda$ value. Then, through the cross-validation, we select the optimal combination of ($\lambda, \alpha_1, \alpha_2$) that produce smallest MSE (or other loss).


```{r,results='hide'}

## Cross validation for choosing the lambda parameter
cv.fit1=cv.hierNest(data$X,data$Y,method="overlapping",# For now, we only wrap-up the overlapping group lasso method in this function
                   hier_info=data$hier_info,family="binomial",
                   partition = "subgroup", # partition = "subgroup" make sure the each n-fold is sampled within the subgroups to avoid extreme cases
                   cvmethod = "general", # cvmethod = "general" indicate the first cross-validation method
                   asparse1=fit1$asparse1,asparse2=fit1$asparse2, ## Should input the tuning parameters asparse1 & asparse2 in order to be consistent with the model "fit1"
                   nlambda = 100,intercept = FALSE)  


## Estimated coefficients for the selected lambda value


# fit1$beta[,order(abs(fit1$lambda-cv.fit1$lambda.min))[1]]

```



### Method 2: grid search cross-validation method 
The second method evenly select several grid points in the (predefined) region of ($\alpha_1, \alpha_2$), each grid point represent a combination of $\alpha_1$ and $\alpha_2$. Then, for each grid point, we run our method with a shorter lambda sequence. We select the optimal value of ($\lambda, \alpha_1, \alpha_2$) that produce smallest MSE (or other loss) through cross-validation.

Note that, this type of cross-validation will generate different lambda sequence compared with the previous fit1 model. Therefore, we need to re-run our method with the selected value of ($\lambda, \alpha_1, \alpha_2$)
```{r,results='hide'}
cv.fit2=cv.hierNest(data$X,data$Y,method="overlapping",# For now, we only wrap-up the overlapping group lasso method in this function
                   hier_info=data$hier_info,family="binomial",
                   partition = "subgroup", # partition = "subgroup" make sure the each n-fold is sampled within the subgroups to avoid extreme cases
                   cvmethod = "grid_search", # cvmethod = "grid_search" indicate the second cross-validation method
                   asparse1 = c(0.5,20),asparse2 = c(0.05,0.20), # For the second method, need to input the upper and lower bounds of alpha_1 and alpha_2
                   asparse1_num = 4,asparse2_num = 4, # number of grids for alpha_1 and alpha_2, total 25 grids will be selected
                   nlambda = 50, # length of lambda sequence for each pair of alpha_1 and alpha_2
                   intercept = FALSE)


fit.selected=hierNest::hierNest(data$X,data$Y,method="overlapping", hier_info=data$hier_info,family="binomial",
                   asparse1 = cv.fit2$sparsegl.fit$asparse1, # Need to input selected alpha_1
                   asparse2 = cv.fit2$sparsegl.fit$asparse2, # Need to input selected alpha_2
                   lambda = cv.fit2$lambda.min, # Selected lambda value
                   intercept = FALSE)

cv.fit2$lambda
```



### Method 3: user supply pairs of asparse1 and asparse2
hierNest also support the user-supplied pairs of $(\alpha_1, \alpha_2)$, see the following code:
```{r}
cv.fit3=hierNest::cv.hierNest(data$X,data$Y,method="overlapping",# For now, we only wrap-up the overlapping group lasso method in this function
                   hier_info=data$hier_info,family="binomial",
                   partition = "subgroup", # partition = "subgroup" make sure the each n-fold is sampled within the subgroups to avoid extreme cases
                   cvmethod = "user_supply", # cvmethod = "grid_search" indicate the second cross-validation method
                   asparse1 = c(1:15),asparse2 = c((1:15)/20), # For the second method, need to input the upper and lower bounds of alpha_1 and alpha_2
                   nlambda = 50,intercept = FALSE)

## selected index of the cross-validation
cv.fit3$lambda[cv.fit3$cv.inx]
cv.fit3$lambda.min

## Final cv.fit, the selected pair of (lambda, alpha_1, alpha_2) can be extracted using cv.inx
cv.fit3$sparsegl.fit

## Selected alpha1
cv.fit3$sparsegl.fit$asparse1[cv.fit3$cv.inx]

## Selected alpha2
cv.fit3$sparsegl.fit$asparse2[cv.fit3$cv.inx]
```


## Exclude intercept can potentially save a huge amount of time

In practice, we observe that including an intercept in the model (by setting `intercept = TRUE`) significantly increases the algorithm's runtime. To address this issue, we have revised our program to incorporate the intercept into the first column of the design matrix, applying no penalty to it. Now, users can set `intercept = FALSE` directly, and our program will automatically generate a column of all 1s, with the coefficient (beta[1,]) representing the overall intercept.

Generally speaking, set `intercept = TRUE` and `intercept = FALSE` is equivalent in the algorithm. However, there may be some differences in the computing. See the following code for comparison:

```{r}
tt1=Sys.time()
fit1.true=hierNest::hierNest(data$X,
                        data$Y,
                        method="overlapping",  ## Overlapping group lasso method
                        hier_info=data$hier_info,  ## Should input the hierarchical information for the groups
                        random_asparse = TRUE,  ## Randomly draw the other two tuning parameter?
                        nlambda=100,
                        intercept = TRUE,  ## Set intercept = FALSE can potentially save a huge amount of time
                        family="binomial")
tt2=Sys.time()
print(tt2-tt1)
fit1.true$npasses


fit1.true$beta[1,]  # When intercept = TRUE, beta[1,] will be 0 as it act the same as intercept
fit1.true$b0 # b0 is the estimate of intercept



tt1=Sys.time()
fit1.false=hierNest::hierNest(data$X,
                        data$Y,
                        method="overlapping",  ## Overlapping group lasso method
                        hier_info=data$hier_info,  ## Should input the hierarchical information for the groups
                        random_asparse = TRUE,  ## Randomly draw the other two tuning parameter?
                        nlambda=100,
                        intercept = FALSE,  ## Set intercept = FALSE can potentially save a huge amount of time
                        family="binomial")
tt2=Sys.time()
print(tt2-tt1)
fit1.false$npasses

fit1.false$beta[1,] # When intercept = FALSE, beta[1,] will be the estimate of intercept
```














